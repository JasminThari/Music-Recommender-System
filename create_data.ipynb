{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_df_from_h5(file, indent=0, datasets={}):\n",
    "    \"\"\"Recursively extracts datasets from an HDF5 file and creates DataFrames for datasets, \n",
    "    decoding byte strings to regular strings.\"\"\"\n",
    "    for key in file:\n",
    "        item = file[key]\n",
    "        print(\"  \" * indent + f\"- {key}: {type(item)}\")\n",
    "        \n",
    "        if isinstance(item, h5py.Group):  \n",
    "            # If it's a group, call the function recursively\n",
    "            extract_df_from_h5(item, indent + 1, datasets)\n",
    "        \n",
    "        elif isinstance(item, h5py.Dataset):  \n",
    "            # If it's a dataset, create the DataFrame\n",
    "            columns = [key for key in item.dtype.names]\n",
    "            df = pd.DataFrame(item[:], columns=columns)\n",
    "            \n",
    "            # Decode byte columns to regular strings if needed\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'O':  # Check if the column has object (likely bytes)\n",
    "                    # Check if the column contains byte strings\n",
    "                    if isinstance(df[col].iloc[0], bytes):  # Only decode if it's bytes\n",
    "                        df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "            # Add the DataFrame to the dictionary with the key as the name of the dataset\n",
    "            datasets[item.name] = df \n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- analysis: <class 'h5py._hl.group.Group'>\n",
      "  - songs: <class 'h5py._hl.dataset.Dataset'>\n",
      "- metadata: <class 'h5py._hl.group.Group'>\n",
      "  - songs: <class 'h5py._hl.dataset.Dataset'>\n",
      "- musicbrainz: <class 'h5py._hl.group.Group'>\n",
      "  - songs: <class 'h5py._hl.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/raw/msd_summary_file.h5'\n",
    "\n",
    "# create dfs from h5 file\n",
    "# takes approx 1-2 min for me (Christoffer)\n",
    "with h5py.File(file_path, 'r') as h5_file:\n",
    "    dfs = extract_df_from_h5(h5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dfs to csv files\n",
    "# approx 30 sec\n",
    "for key in dfs:\n",
    "    name = key.split(\"/\")[1]\n",
    "    dfs[key].to_csv(f\"data/{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new df with the track_id and song_id such that we can map between the two\n",
    "df_unique_tracks = pd.read_csv(\"data/raw/unique_tracks.txt\", sep='<SEP>', header=None, engine='python')\n",
    "df_unique_tracks.columns = ['track_id', 'song_id', 'artist', 'title']\n",
    "# we choose to drop duplicates based on the song_id, as we assume that the song_id is the unique identifier\n",
    "# note we have 999056 unique song_id's in the unique_tracks.txt file\n",
    "# so the potential loss of data is minimal\n",
    "# further more in the user data, it is the song_id that is used to identify the songs\n",
    "# we will use the first track_id that a song_id is mapped to\n",
    "song_to_track_mapping = df_unique_tracks[['song_id', 'track_id']].drop_duplicates(subset='song_id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = dfs['/metadata/songs'] # has song_id\n",
    "# drop duplicates based on song_id, same logic as above cell\n",
    "metadata_df = metadata_df.drop_duplicates(subset='song_id', keep='first')\n",
    "analysis_df = dfs['/analysis/songs'] # has track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = metadata_df.merge(song_to_track_mapping, on='song_id', how='inner')\n",
    "merged_df = merged_df.merge(analysis_df, on='track_id', how='inner')\n",
    "# remove all unnecessary columns\n",
    "# we remove all unnecessary identifiers, as we have the song_id and track_id\n",
    "# we also remove all columns with only one unique value, as they do not provide any information\n",
    "merged_df = merged_df.drop(columns=[\"artist_7digitalid\",\n",
    "                                    \"artist_mbid\",\n",
    "                                    \"artist_playmeid\",\n",
    "                                    \"release_7digitalid\",\n",
    "                                    \"track_7digitalid\",\n",
    "                                    \"audio_md5\",])\n",
    "\n",
    "# we also remove all columns with only one unique value, as they do not provide any valuable information\n",
    "merged_df = merged_df.loc[:, merged_df.nunique() > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genres_cd1 = pd.read_csv(\n",
    "    \"data/raw/msd_tagtraum_cd1.cls\",\n",
    "    sep=\"\\t\",                   # Tab-separated values\n",
    "    comment='#',                # Ignore lines starting with '#'\n",
    "    header=None,                # No predefined header row\n",
    "    names=[\"trackId\", \"majority_genre\", \"minority_genre\"],  # Define column names\n",
    "    engine='python'             # To handle varying column counts\n",
    ")\n",
    "df_genres_cd2 = pd.read_csv(\n",
    "    \"data/raw/msd_tagtraum_cd2.cls\",\n",
    "    sep=\"\\t\",                   # Tab-separated values\n",
    "    comment='#',                # Ignore lines starting with '#'\n",
    "    header=None,                # No predefined header row\n",
    "    names=[\"trackId\", \"majority_genre\", \"minority_genre\"],  # Define column names\n",
    "    engine='python'             # To handle varying column counts\n",
    ")\n",
    "df_genres_cd2c = pd.read_csv(\n",
    "    \"data/raw/msd_tagtraum_cd2c.cls\",\n",
    "    sep=\"\\t\",                   # Tab-separated values\n",
    "    comment='#',                # Ignore lines starting with '#'\n",
    "    header=None,                # No predefined header row\n",
    "    names=[\"trackId\", \"majority_genre\", \"minority_genre\"],  # Define column names\n",
    "    engine='python'             # To handle varying column counts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find trackIds in df_genres_cd2 that are not in df_genres_cd1\n",
    "new_trackIds_cd2 = df_genres_cd2[~df_genres_cd2['trackId'].isin(df_genres_cd1['trackId'])]\n",
    "# Add these new trackIds to df_genres_cd1\n",
    "df_genres_cd1_updated = pd.concat([df_genres_cd1, new_trackIds_cd2], ignore_index=True)\n",
    "\n",
    "# Find trackIds in df_genres_cd2c that are not in df_genres_cd1_updated\n",
    "new_trackIds_cd2c = df_genres_cd2c[~df_genres_cd2c['trackId'].isin(df_genres_cd1_updated['trackId'])]\n",
    "# Add these new trackIds to df_genres_cd1_updated\n",
    "df_genres_merged = pd.concat([df_genres_cd1_updated, new_trackIds_cd2c], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the genre information to the merged_df, \n",
    "# we use information from both majority_genre and minority_genre and create two columns for each genre dataset\n",
    "\n",
    "\n",
    "merged_df = merged_df.merge(df_genres_cd1, left_on='track_id', right_on='trackId', how='left')\n",
    "# remove the trackId column as it is redundant and just confuses\n",
    "merged_df = merged_df.drop(columns=['trackId'])\n",
    "# rename the columns to reflect the source of the data\n",
    "merged_df = merged_df.rename(columns={\"majority_genre\": \"majority_genre_cd1\", \"minority_genre\": \"minority_genre_cd1\"})\n",
    "\n",
    "# do the same for the other genre datasets\n",
    "merged_df = merged_df.merge(df_genres_cd2, left_on='track_id', right_on='trackId', how='left')\n",
    "merged_df = merged_df.drop(columns=['trackId'])\n",
    "merged_df = merged_df.rename(columns={\"majority_genre\": \"majority_genre_cd2\", \"minority_genre\": \"minority_genre_cd2\"})\n",
    "\n",
    "merged_df = merged_df.merge(df_genres_cd2c, left_on='track_id', right_on='trackId', how='left')\n",
    "merged_df = merged_df.drop(columns=['trackId'])\n",
    "merged_df = merged_df.rename(columns={\"majority_genre\": \"majority_genre_cd2c\", \"minority_genre\": \"minority_genre_cd2c\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approx 20 sec\n",
    "# order the columns in alphabetical order\n",
    "merged_df = merged_df.reindex(sorted(merged_df.columns), axis=1)\n",
    "# save the merged_df to a csv file\n",
    "merged_df.to_csv(\"data/all_songs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete memory to free up space\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approx 1 min\n",
    "import pandas as pd\n",
    "# load the user data and find the unique song_id's\n",
    "df_user_data = pd.read_csv(\"data/raw/user_data.txt\", sep='\\t', header=None)\n",
    "df_user_data.columns = ['user_id', 'song_id', 'play_count']\n",
    "# find the unique song_id's that has been played\n",
    "unique_song_ids = df_user_data['song_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the user data to free up space, since it is not needed anymore\n",
    "del df_user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approx 20 sec\n",
    "# load the all_songs data\n",
    "df_all_songs = pd.read_csv(\"Data/all_songs.csv\")\n",
    "# find the songs that are in the user data\n",
    "df_played_songs = df_all_songs.merge(pd.DataFrame(unique_song_ids, columns=['song_id']), on='song_id', how='inner')\n",
    "# save the data\n",
    "df_played_songs.to_csv(\"Data/played_songs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_data import DataSetMaker\n",
    "from make_data import DataLoader\n",
    "\n",
    "def get_data():\n",
    "    # Define file paths\n",
    "    h5_file = 'data/raw/msd_summary_file.h5'\n",
    "    unique_tracks = 'data/raw/unique_tracks.txt'\n",
    "    genres_files = [\n",
    "        'data/raw/msd_tagtraum_cd1.cls',\n",
    "        'data/raw/msd_tagtraum_cd2.cls',\n",
    "        'data/raw/msd_tagtraum_cd2c.cls'\n",
    "    ]\n",
    "    user_data = 'data/raw/user_data.txt'\n",
    "    output_directory = 'data'\n",
    "\n",
    "    # Initialize the DataSetMaker\n",
    "    dataset_maker = DataSetMaker(\n",
    "        h5_file_path=h5_file,\n",
    "        unique_tracks_path=unique_tracks,\n",
    "        genres_paths=genres_files,\n",
    "        user_data_path=user_data,\n",
    "        output_dir=output_directory\n",
    "    )\n",
    "\n",
    "    # Run all processing steps\n",
    "    dataset_maker.run_all()\n",
    "\n",
    "\n",
    "# get_data()\n",
    "\n",
    "data_loader = DataLoader()\n",
    "df_songs = data_loader.load_song_data(data_path='data/played_songs.csv')\n",
    "df_user = data_loader.load_user_data(data_path='data/raw/user_data.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SGI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
