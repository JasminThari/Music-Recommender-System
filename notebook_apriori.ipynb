{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, tril\n",
    "from scipy.special import comb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW RECOMMENDATION FUNCTION ###\n",
    "def evaluate_recommendations(recommendations_df, masked_songs_df, list_of_ks = [10,20,30,40,50]):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendations using Recall@k.\n",
    "    Each user should prefable have 50 recommendations.\n",
    "    \n",
    "    Parameters:\n",
    "    - recommendations_df (pd.DataFrame): DataFrame with 'user_id' and 'recommended_songs' (list of song_ids).\n",
    "    - masked_songs_df (pd.DataFrame): DataFrame with 'user_id' and 'song_id' of masked songs.\n",
    "    - ks (list): List of integers with the values of k to compute Recall@k.\n",
    "    \n",
    "    Returns:\n",
    "    - evaluation_df (pd.DataFrame): DataFrame with 'user_id', and 'k_i' columns for each k in ks.   \n",
    "    \"\"\"\n",
    "    # Ensure the recommended_songs are lists\n",
    "    recommendations_df['recommended_songs'] = recommendations_df['recommended_songs'].apply(list)\n",
    "    \n",
    "    # Group masked songs by user\n",
    "    masked_songs_grouped = masked_songs_df.groupby('user_id')['song_id'].apply(set).reset_index()\n",
    "    masked_songs_dict = dict(zip(masked_songs_grouped['user_id'], masked_songs_grouped['song_id']))\n",
    "    \n",
    "    evaluation_results = []\n",
    "    \n",
    "    for _, row in recommendations_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        # Get the masked songs for the user\n",
    "        masked_songs = masked_songs_dict.get(user_id, set())\n",
    "        recalls = []\n",
    "        for k in list_of_ks:\n",
    "            recommended_songs = row['recommended_songs'][:k]   \n",
    "            if not masked_songs:\n",
    "                # If there are no masked songs for the user, we cannot compute recall\n",
    "                recall_at_k = None\n",
    "            else:\n",
    "                # Compute the number of relevant recommended songs\n",
    "                relevant_recommendations = set(recommended_songs) & masked_songs\n",
    "                num_relevant = len(relevant_recommendations)\n",
    "\n",
    "                # Recall@k\n",
    "                recall_at_k = num_relevant / len(masked_songs) if len(masked_songs) > 0 else 0\n",
    "\n",
    "            recalls.append(recall_at_k)\n",
    "\n",
    "        recall_dict = {f\"k_{k}\": recall_at_k for k, recall_at_k in zip(list_of_ks, recalls)}\n",
    "\n",
    "        evaluation_results.append({\n",
    "            'user_id': user_id,\n",
    "            **recall_dict\n",
    "        })\n",
    "\n",
    "    evaluation_df = pd.DataFrame(evaluation_results)\n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ALL_USERS_DATA = \"data/processed/user_data_cleaned.csv\"\n",
    "PATH_TRAIN_USERS_IDS = \"data/processed/train_users.csv\"\n",
    "PATH_TRAIN_USERS_DATA = \"data/processed/user_data_clean_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_train_users(path_to_all_users_data: str, path_to_train_users_IDs: str, output_path: str) -> pd.DataFrame:\n",
    "    \n",
    "    # Read the data files\n",
    "    listened_songs_all_users = pd.read_csv(path_to_all_users_data)\n",
    "    train_users = pd.read_csv(path_to_train_users_IDs)[\"user_id\"]\n",
    "    # Filter the dataframe to include only users in the training set\n",
    "    train_users_data = listened_songs_all_users[listened_songs_all_users[\"user_id\"].isin(train_users)]\n",
    "    # Save the filtered dataframe    \n",
    "    train_users_data.to_csv(output_path, index=False)\n",
    "    # Print the number of unique users\n",
    "    print(f\"Number of unique users in the training set: {train_users_data['user_id'].nunique()}\")\n",
    "    print(f\"Number of unique songs in the training set: {train_users_data['song_id'].nunique()}\")\n",
    "    return train_users_data   \n",
    "\n",
    "def create_sparse_matrix(path_to_train_users_data: str, transaction_matrix: bool = True):\n",
    "    \"\"\"\n",
    "    Create a sparse matrix from a csv file with user data with columns song_id, user_id, play_count    \n",
    "    Input:\n",
    "    path_to_user_data: str, path to the user data\n",
    "    transaction_matrix: bool, if True the play_count is set to 1, else the play_count is used as is\n",
    "    Returns:\n",
    "    sparse_matrix: csr_matrix, \n",
    "    user_id_mapping: dict, mapping from the integer codes to the original user ids\n",
    "    song_id_mapping: dict, mapping from the integer codes to the original song ids\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path_to_train_users_data)\n",
    "    # Convert user_id and song_id to categories\n",
    "    # this the pandas way of mapping strings as integers    \n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(\"category\")\n",
    "    df[\"song_id\"] = df[\"song_id\"].astype(\"category\")\n",
    "\n",
    "    # Extract the integer codes from the categories\n",
    "    user_codes = df[\"user_id\"].cat.codes\n",
    "    song_codes = df[\"song_id\"].cat.codes\n",
    "\n",
    "    # mapping from the integer codes to the original strings\n",
    "    user_id_mapping = dict(enumerate(df[\"user_id\"].cat.categories))\n",
    "    song_id_mapping = dict(enumerate(df[\"song_id\"].cat.categories))\n",
    "\n",
    "    \n",
    "    if transaction_matrix:\n",
    "        df[\"play_count\"] = 1  \n",
    "        # create of type bool to save memory      \n",
    "        sparse_matrix = csr_matrix(\n",
    "            (df[\"play_count\"], (user_codes, song_codes)),\n",
    "            shape=(df[\"user_id\"].cat.categories.size, df[\"song_id\"].cat.categories.size),\n",
    "            dtype=\"bool\"\n",
    "        )\n",
    "    else:\n",
    "        sparse_matrix = csr_matrix(\n",
    "            (df[\"play_count\"], (user_codes, song_codes)),\n",
    "            shape=(df[\"user_id\"].cat.categories.size, df[\"song_id\"].cat.categories.size),\n",
    "            dtype=\"int32\"\n",
    "        )\n",
    "        \n",
    "    print(f\"Created sparse matrix wih shape: {sparse_matrix.shape}\")\n",
    "    print(f\"Memory usage of sparse matrix: {sparse_matrix.data.nbytes / 1024:.2f} KB\")\n",
    "    return sparse_matrix , user_id_mapping, song_id_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in the training set: 156236\n",
      "Number of unique songs in the training set: 62066\n",
      "Created sparse matrix wih shape: (156236, 62066)\n",
      "Memory usage of sparse matrix: 6432.13 KB\n"
     ]
    }
   ],
   "source": [
    "train_users_data = find_train_users(PATH_ALL_USERS_DATA, PATH_TRAIN_USERS_IDS, PATH_TRAIN_USERS_DATA)\n",
    "transaction_matrix, user_id_mapping, song_id_mapping = create_sparse_matrix(PATH_TRAIN_USERS_DATA, transaction_matrix=True)\n",
    "TOTAL_NUMBER_OF_USERS = transaction_matrix.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_algorithm_pairs(crs_matrix: csr_matrix, min_support: float, write_to_file: bool = True):\n",
    "                            \n",
    "    \"\"\"\n",
    "    Apriori algorithm for pairs using crs_matrix and matrix multiplication\n",
    "    Input: \n",
    "    crs_matrix: sparse matrix, the transaction matrix\n",
    "    min_support: float, the minimum support\n",
    "    write_to_file: bool, if True the frequent itemsets are written to a file\n",
    "    \"\"\"\n",
    "    TOTAL_NUMBER_OF_USERS = crs_matrix.shape[0]\n",
    "    SUPPORT_THRESHOLD = int(math.ceil(TOTAL_NUMBER_OF_USERS * min_support))\n",
    "    WRITE_TO_FILE = write_to_file\n",
    "    out_folder = \"results\"\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    OUT_PATH = f\"{out_folder}/frequent_itemsets_support_{min_support}.csv\"\n",
    "    # delete the file if it already exists\n",
    "    if WRITE_TO_FILE and os.path.exists(OUT_PATH):\n",
    "        os.remove(OUT_PATH)  \n",
    "    \n",
    "    # singletons           \n",
    "    single_item_occurance = np.array(crs_matrix.sum(axis=0)).flatten()\n",
    "    relevant_single_item_indices = np.where(single_item_occurance >= SUPPORT_THRESHOLD)[0]\n",
    "    print(f\"Number of single items above the threshold: {len(relevant_single_item_indices)}\")\n",
    "    filtered_csr_idx_to_org_item_idx = {new_index: old_index for new_index, old_index in enumerate(relevant_single_item_indices)}\n",
    "    filtered_crs_matrix = crs_matrix[:,relevant_single_item_indices]\n",
    "    # remove old matrix to free up memory\n",
    "    frequent_singletons = {i: single_item_occurance[i] for i in relevant_single_item_indices}\n",
    "\n",
    "    if WRITE_TO_FILE:\n",
    "        with open(OUT_PATH, \"a\") as f:                \n",
    "            for singleton, frequency in frequent_singletons.items():\n",
    "                support = frequency / TOTAL_NUMBER_OF_USERS                             \n",
    "                f.write(f\"({singleton}),{support}\\n\")\n",
    "\n",
    "    # pairs\n",
    "    pairwise_occurance_matrix = filtered_crs_matrix.astype(np.int32).T @ filtered_crs_matrix.astype(np.int32)\n",
    "    # only extract the lower triangle (excluding the diagonal) since the matrix is symmetric\n",
    "    pairwise_occurance_matrix = tril(pairwise_occurance_matrix, k=-1)\n",
    "    coo = pairwise_occurance_matrix.tocoo()\n",
    "    mask = coo.data >= SUPPORT_THRESHOLD\n",
    "    filtered_rows, filtered_cols, filtered_values = coo.row[mask], coo.col[mask], coo.data[mask]\n",
    "\n",
    "    original_rows = [filtered_csr_idx_to_org_item_idx[i] for i in filtered_rows]\n",
    "    original_cols = [filtered_csr_idx_to_org_item_idx[i] for i in filtered_cols]    \n",
    "\n",
    "    frequent_pairs  = {}\n",
    "    for row, col, value in zip(original_rows, original_cols, filtered_values):\n",
    "        item_set = tuple(sorted((row, col)))\n",
    "        assert item_set not in frequent_pairs, f\"Key {item_set} is already in the dictionary, should not happen\"            \n",
    "        frequent_pairs[item_set] = value\n",
    "\n",
    "    print(f\"Number of pairs above the threshold: {len(frequent_pairs)}\")\n",
    "    if WRITE_TO_FILE:\n",
    "        with open(OUT_PATH, \"a\") as f:                \n",
    "            for itemset in frequent_pairs:\n",
    "                # Ensure valid mapping of filtered indices to original item indices\n",
    "                #original_item_indices = tuple(sorted([filtered_csr_idx_to_org_item_idx[i] for i in itemset]))                   \n",
    "                support = frequent_pairs[itemset] / TOTAL_NUMBER_OF_USERS\n",
    "                f.write(f\"{itemset},{support}\\n\")        \n",
    "\n",
    "    return frequent_singletons, frequent_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of single items above the threshold: 9228\n",
      "Number of pairs above the threshold: 47748\n"
     ]
    }
   ],
   "source": [
    "MIN_SUPPORT = 0.001\n",
    "freq_singletons, freq_pairs = apriori_algorithm_pairs(transaction_matrix, min_support=MIN_SUPPORT, write_to_file=True)\n",
    "\n",
    "support_singletons = {item: freq_singletons[item] / TOTAL_NUMBER_OF_USERS for item in freq_singletons}\n",
    "support_pairs = {item: freq_pairs[item] / TOTAL_NUMBER_OF_USERS for item in freq_pairs}\n",
    "\n",
    "singleton_df = pd.DataFrame({\"item\": list(support_singletons.keys()), \"support\": list(support_singletons.values())})\n",
    "pairs_df = pd.DataFrame({\"itemset\": list(support_pairs.keys()), \"pair_support\": list(support_pairs.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence(support_A_and_B, support_A):\n",
    "    return support_A_and_B / support_A\n",
    "\n",
    "def calculate_lift(confidence_A_to_B, support_B):\n",
    "    return confidence_A_to_B / support_B\n",
    "\n",
    "pairs_df[\"item_A\"] = pairs_df[\"itemset\"].apply(lambda x: x[0])\n",
    "pairs_df[\"item_B\"] = pairs_df[\"itemset\"].apply(lambda x: x[1])\n",
    "pairs_df[\"support_A\"] = pairs_df[\"item_A\"].apply(lambda x: support_singletons[x])\n",
    "pairs_df[\"support_B\"] = pairs_df[\"item_B\"].apply(lambda x: support_singletons[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemset</th>\n",
       "      <th>pair_support</th>\n",
       "      <th>item_A</th>\n",
       "      <th>item_B</th>\n",
       "      <th>support_A</th>\n",
       "      <th>support_B</th>\n",
       "      <th>confidence_A_to_B</th>\n",
       "      <th>confidence_B_to_A</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(14, 55080)</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>14</td>\n",
       "      <td>55080</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>0.572973</td>\n",
       "      <td>0.157153</td>\n",
       "      <td>66.359530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(15, 42101)</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>15</td>\n",
       "      <td>42101</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.309951</td>\n",
       "      <td>97.829321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(15, 58716)</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>15</td>\n",
       "      <td>58716</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.361616</td>\n",
       "      <td>0.389130</td>\n",
       "      <td>122.820571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(18, 39893)</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>18</td>\n",
       "      <td>39893</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.146378</td>\n",
       "      <td>0.280401</td>\n",
       "      <td>32.717449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(18, 53681)</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>18</td>\n",
       "      <td>53681</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.194922</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>67.675041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       itemset  pair_support  item_A  item_B  support_A  support_B   \n",
       "0  (14, 55080)      0.001357      14   55080   0.002368   0.008634  \\\n",
       "1  (15, 42101)      0.001216      15   42101   0.003168   0.003924   \n",
       "2  (15, 58716)      0.001146      15   58716   0.003168   0.002944   \n",
       "3  (18, 39893)      0.001255      18   39893   0.008570   0.004474   \n",
       "4  (18, 53681)      0.001671      18   53681   0.008570   0.002880   \n",
       "\n",
       "   confidence_A_to_B  confidence_B_to_A        lift  \n",
       "0           0.572973           0.157153   66.359530  \n",
       "1           0.383838           0.309951   97.829321  \n",
       "2           0.361616           0.389130  122.820571  \n",
       "3           0.146378           0.280401   32.717449  \n",
       "4           0.194922           0.580000   67.675041  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df[\"confidence_A_to_B\"] = pairs_df.apply(\n",
    "    lambda x: calculate_confidence(x[\"pair_support\"], x[\"support_A\"]), axis=1\n",
    ")\n",
    "pairs_df[\"confidence_B_to_A\"] = pairs_df.apply(\n",
    "    lambda x: calculate_confidence(x[\"pair_support\"], x[\"support_B\"]), axis=1)\n",
    "\n",
    "pairs_df[\"lift\"] = pairs_df.apply(\n",
    "    lambda x: calculate_lift(x[\"confidence_A_to_B\"], x[\"support_B\"]), axis=1\n",
    ")\n",
    "pairs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved measurements to results/measurements_0.001.json\n"
     ]
    }
   ],
   "source": [
    "# save to json file:\n",
    "measurement_dict = {}\n",
    "\n",
    "for _, row in pairs_df.iterrows():\n",
    "    item1, item2 = song_id_mapping[row['item_A']], song_id_mapping[row['item_B']]\n",
    "    conf1_to_2 = row['confidence_A_to_B']\n",
    "    conf2_to_1 = row['confidence_B_to_A']\n",
    "    lift = row['lift'] # remember lift_A_to_B == lift_B_to_A    \n",
    "    # Add item1 -> item2 confidence\n",
    "    if item1 not in measurement_dict:\n",
    "        measurement_dict[item1] = {item2: {\"confidence\": conf1_to_2, \"lift\": lift}}   \n",
    "    else:\n",
    "        measurement_dict[item1][item2] = {\"confidence\": conf1_to_2, \"lift\": lift}\n",
    "    \n",
    "    # Add item2 -> item1 confidence\n",
    "    if item2 not in measurement_dict:\n",
    "        measurement_dict[item2] = {item1: {\"confidence\": conf2_to_1, \"lift\": lift}}   \n",
    "    else:\n",
    "        measurement_dict[item2][item1] = {\"confidence\": conf2_to_1, \"lift\": lift}\n",
    "\n",
    "# Save to JSON file\n",
    "with open(f\"results/measurements_{MIN_SUPPORT}.json\", \"w\") as json_file:\n",
    "    json.dump(measurement_dict, json_file, indent=4)\n",
    "\n",
    "print(f\"Saved measurements to results/measurements_{MIN_SUPPORT}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure the data such that the confidence and lift values are sorted in descending order\n",
    "\n",
    "def restructure_data(data):\n",
    "    result = {}\n",
    "    \n",
    "    for song, related_songs in data.items():\n",
    "        # Collect confidence and lift values for each song\n",
    "        confidence_list = sorted(\n",
    "            [(related, values['confidence']) for related, values in related_songs.items()],\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        lift_list = sorted(\n",
    "            [(related, values['lift']) for related, values in related_songs.items()],\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        \n",
    "        result[song] = {\n",
    "            'confidence': confidence_list,\n",
    "            'lift': lift_list\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "restructured_data = restructure_data(measurement_dict)\n",
    "# save to json file:\n",
    "with open(f\"results/measurements_restructured_{MIN_SUPPORT}.json\", \"w\") as json_file:\n",
    "    json.dump(restructured_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TEST_DATA_FIXED = \"data/processed/listened_songs_fixed_split.csv\"\n",
    "PATH_TEST_MASKED_FIXED = \"data/processed/masked_songs_fixed_split.csv\"\n",
    "\n",
    "\n",
    "test_users_data_fixed = pd.read_csv(PATH_TEST_DATA_FIXED)\n",
    "test_users_data_masked_fixed = pd.read_csv(PATH_TEST_MASKED_FIXED)\n",
    "\n",
    "\n",
    "def group_data(data):\n",
    "    \"\"\"\n",
    "    Group the data by user_id\n",
    "    Input:\n",
    "    data: pd.DataFrame, the data to group\n",
    "    Returns:\n",
    "    grouped_data: dict, the grouped data\n",
    "    \"\"\"\n",
    "    grouped_data = data.groupby(\"user_id\")[\"song_id\"].apply(list).reset_index()\n",
    "    grouped_data.columns = ['user_id', 'songs']\n",
    "    return grouped_data\n",
    "\n",
    "test_users_data_fixed = group_data(test_users_data_fixed)\n",
    "test_users_data_masked_fixed = group_data(test_users_data_masked_fixed)\n",
    "test_users_data_fixed = test_users_data_fixed.merge(test_users_data_masked_fixed, on=\"user_id\", suffixes=('_listened', '_masked'))\n",
    "test_users_data_fixed[\"num_songs_to_recommend\"] = test_users_data_fixed[\"songs_masked\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs(restructured_data,known_songs,k,measurement_type='confidence',verbose=False):\n",
    "    \"\"\"\n",
    "    Recommend k songs based on the known songs\n",
    "    Input:\n",
    "    restructured_data: dict, the restructured data that is sorted in descending order\n",
    "    known_songs: list, the songs the user has listened to\n",
    "    k: int, the number of songs to recommend\n",
    "    measurement_type: str, the type of measurement to use, either 'confidence' or 'lift'\n",
    "    Returns:\n",
    "    recommended_songs: list, the recommended songs\n",
    "    \"\"\"\n",
    "    # Collect the confidence values for the known songs    \n",
    "    all_possible_recommendations = []\n",
    "    for song in known_songs:\n",
    "        try:\n",
    "            related_songs = restructured_data[song][measurement_type]\n",
    "            all_possible_recommendations.extend(related_songs)\n",
    "        except KeyError:\n",
    "            if verbose:\n",
    "                print(f\"Song {song} is not in the dataset\")\n",
    "            continue\n",
    "    # Sort the confidence values in descending order\n",
    "    all_possible_recommendations = sorted(all_possible_recommendations, key=lambda x: x[1], reverse=True)\n",
    "    # Collect the top k songs\n",
    "    recommended_songs = [song_and_score for song_and_score in all_possible_recommendations[:k]]\n",
    "    return recommended_songs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_remove_cols(df_in):\n",
    "    #Make a copy of the dataframe\n",
    "    df = df_in.copy()\n",
    "    df[\"recommended_songs\"] = df[\"recommended_songs_tuple\"].apply(lambda x: [song for song, _ in x])\n",
    "    df[\"scores\"] = df[\"recommended_songs_tuple\"].apply(lambda x: [confidence for _, confidence in x])\n",
    "    df[\"probabilities\"] = df[\"scores\"].apply(lambda x: np.exp(x) / np.sum(np.exp(x)))\n",
    "    # keep columns ['user_id', 'recommended_songs', 'recommended_songs_score', 'softmax_score']\n",
    "    df = df[['user_id', 'recommended_songs', 'scores', 'probabilities']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurement type: confidence\n",
      "Measurement type: lift\n"
     ]
    }
   ],
   "source": [
    "# recommend songs for different measurement types and recommendation strategies\n",
    "# also evaluate the recommendations\n",
    "NUM_SONGS_TO_RECOMMEND = 50\n",
    "\n",
    "test_labels_masked_fixed = pd.read_csv(PATH_TEST_MASKED_FIXED)\n",
    "\n",
    "for measurement_type in ['confidence', 'lift']:    \n",
    "    print(f\"Measurement type: {measurement_type}\")    \n",
    "    test_users_data_fixed[\"recommended_songs_tuple\"] = test_users_data_fixed.apply(\n",
    "        lambda row: recommend_songs(restructured_data, \n",
    "                                    row[\"songs_listened\"],\n",
    "                                    NUM_SONGS_TO_RECOMMEND,                                       \n",
    "                                    measurement_type=measurement_type), axis=1)                                    \n",
    "    output_fixed = add_and_remove_cols(test_users_data_fixed)\n",
    "    output_fixed.to_csv(f\"results/recommendations_fixed_{measurement_type}_{MIN_SUPPORT}.csv\", index=False)\n",
    "    evaluation_fixed = evaluate_recommendations(output_fixed, \n",
    "                                                test_labels_masked_fixed) \n",
    "                                                \n",
    "    evaluation_fixed.to_csv(f\"results/evaluation_fixed_{measurement_type}_{MIN_SUPPORT}.csv\", index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>support</th>\n",
       "      <th>k_10</th>\n",
       "      <th>k_20</th>\n",
       "      <th>k_30</th>\n",
       "      <th>k_40</th>\n",
       "      <th>k_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>confidence</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.106282</td>\n",
       "      <td>0.164638</td>\n",
       "      <td>0.207029</td>\n",
       "      <td>0.239925</td>\n",
       "      <td>0.267666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lift</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.053043</td>\n",
       "      <td>0.085162</td>\n",
       "      <td>0.111591</td>\n",
       "      <td>0.134085</td>\n",
       "      <td>0.153888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confidence</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.098575</td>\n",
       "      <td>0.149503</td>\n",
       "      <td>0.185855</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.236911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lift</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.062324</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>0.126052</td>\n",
       "      <td>0.149938</td>\n",
       "      <td>0.170158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>confidence</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.089515</td>\n",
       "      <td>0.133607</td>\n",
       "      <td>0.164076</td>\n",
       "      <td>0.187038</td>\n",
       "      <td>0.205527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lift</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.063474</td>\n",
       "      <td>0.098480</td>\n",
       "      <td>0.125149</td>\n",
       "      <td>0.146718</td>\n",
       "      <td>0.165175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       method  support      k_10      k_20      k_30      k_40      k_50\n",
       "0  confidence   0.0001  0.106282  0.164638  0.207029  0.239925  0.267666\n",
       "1        lift   0.0001  0.053043  0.085162  0.111591  0.134085  0.153888\n",
       "2  confidence   0.0005  0.098575  0.149503  0.185855  0.213800  0.236911\n",
       "3        lift   0.0005  0.062324  0.098100  0.126052  0.149938  0.170158\n",
       "4  confidence   0.0010  0.089515  0.133607  0.164076  0.187038  0.205527\n",
       "5        lift   0.0010  0.063474  0.098480  0.125149  0.146718  0.165175"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supports = [0.001, 0.0005, 0.0001]\n",
    "methods = ['confidence', 'lift']\n",
    "\n",
    "data = []\n",
    "for support in supports:\n",
    "    for method in methods:\n",
    "        file = f\"results/evaluation_fixed_{method}_{support}.csv\"\n",
    "        mean_values = pd.read_csv(file).drop(columns=[\"user_id\"]).mean()\n",
    "        mean_values[\"method\"] = method\n",
    "        mean_values[\"support\"] = support\n",
    "        data.append(mean_values)\n",
    "        \n",
    "#final creation\n",
    "final_df = pd.DataFrame(data).reset_index(drop=True)\n",
    "# sort by support and method\n",
    "final_df = final_df.sort_values(by=[\"support\", \"method\"]).reset_index(drop=True)\n",
    "# make method and support the first columns\n",
    "final_df = final_df[[\"method\", \"support\"] + [col for col in final_df.columns if col not in [\"method\", \"support\"]]]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>support</th>\n",
       "      <th>k_10</th>\n",
       "      <th>k_20</th>\n",
       "      <th>k_30</th>\n",
       "      <th>k_40</th>\n",
       "      <th>k_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>confidence</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.106282</td>\n",
       "      <td>0.164638</td>\n",
       "      <td>0.207029</td>\n",
       "      <td>0.239925</td>\n",
       "      <td>0.267666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lift</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.053043</td>\n",
       "      <td>0.085162</td>\n",
       "      <td>0.111591</td>\n",
       "      <td>0.134085</td>\n",
       "      <td>0.153888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confidence</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.098575</td>\n",
       "      <td>0.149503</td>\n",
       "      <td>0.185855</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.236911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lift</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.062324</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>0.126052</td>\n",
       "      <td>0.149938</td>\n",
       "      <td>0.170158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>confidence</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.089515</td>\n",
       "      <td>0.133607</td>\n",
       "      <td>0.164076</td>\n",
       "      <td>0.187038</td>\n",
       "      <td>0.205527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lift</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.063474</td>\n",
       "      <td>0.098480</td>\n",
       "      <td>0.125149</td>\n",
       "      <td>0.146718</td>\n",
       "      <td>0.165175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       method  support      k_10      k_20      k_30      k_40      k_50\n",
       "0  confidence   0.0001  0.106282  0.164638  0.207029  0.239925  0.267666\n",
       "1        lift   0.0001  0.053043  0.085162  0.111591  0.134085  0.153888\n",
       "2  confidence   0.0005  0.098575  0.149503  0.185855  0.213800  0.236911\n",
       "3        lift   0.0005  0.062324  0.098100  0.126052  0.149938  0.170158\n",
       "4  confidence   0.0010  0.089515  0.133607  0.164076  0.187038  0.205527\n",
       "5        lift   0.0010  0.063474  0.098480  0.125149  0.146718  0.165175"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComSocSci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
