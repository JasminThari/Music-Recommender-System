{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, tril\n",
    "from scipy.special import comb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW RECOMMENDATION FUNCTION ###\n",
    "def evaluate_recommendations(recommendations_df, masked_songs_df, list_of_ks = [10,20,30,40,50]):\n",
    "    \"\"\"\n",
    "    Evaluate the recommendations using Recall@k.\n",
    "    Each user should prefable have 50 recommendations.\n",
    "    \n",
    "    Parameters:\n",
    "    - recommendations_df (pd.DataFrame): DataFrame with 'user_id' and 'recommended_songs' (list of song_ids).\n",
    "    - masked_songs_df (pd.DataFrame): DataFrame with 'user_id' and 'song_id' of masked songs.\n",
    "    - ks (list): List of integers with the values of k to compute Recall@k.\n",
    "    \n",
    "    Returns:\n",
    "    - evaluation_df (pd.DataFrame): DataFrame with 'user_id', and 'k_i' columns for each k in ks.   \n",
    "    \"\"\"\n",
    "    # Ensure the recommended_songs are lists\n",
    "    recommendations_df['recommended_songs'] = recommendations_df['recommended_songs'].apply(list)\n",
    "    \n",
    "    # Group masked songs by user\n",
    "    masked_songs_grouped = masked_songs_df.groupby('user_id')['song_id'].apply(set).reset_index()\n",
    "    masked_songs_dict = dict(zip(masked_songs_grouped['user_id'], masked_songs_grouped['song_id']))\n",
    "    \n",
    "    evaluation_results = []\n",
    "    \n",
    "    for _, row in recommendations_df.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        # Get the masked songs for the user\n",
    "        masked_songs = masked_songs_dict.get(user_id, set())\n",
    "        recalls = []\n",
    "        for k in list_of_ks:\n",
    "            recommended_songs = row['recommended_songs'][:k]   \n",
    "            if not masked_songs:\n",
    "                # If there are no masked songs for the user, we cannot compute recall\n",
    "                recall_at_k = None\n",
    "            else:\n",
    "                # Compute the number of relevant recommended songs\n",
    "                relevant_recommendations = set(recommended_songs) & masked_songs\n",
    "                num_relevant = len(relevant_recommendations)\n",
    "\n",
    "                # Recall@k\n",
    "                recall_at_k = num_relevant / len(masked_songs) if len(masked_songs) > 0 else 0\n",
    "\n",
    "            recalls.append(recall_at_k)\n",
    "\n",
    "        recall_dict = {f\"k_{k}\": recall_at_k for k, recall_at_k in zip(list_of_ks, recalls)}\n",
    "\n",
    "        evaluation_results.append({\n",
    "            'user_id': user_id,\n",
    "            **recall_dict\n",
    "        })\n",
    "\n",
    "    evaluation_df = pd.DataFrame(evaluation_results)\n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Apriori Algorithm\n",
    "\n",
    "In this section, weâ€™ll use the Apriori Algorithm to identify frequent itemsets of size 2.\n",
    "Based on the frequent itemsets, we will generate association rules to recommend songs to users based on the songs they have listened to. The association rules used are confidence and lift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Sparse Matrix Representation\n",
    "\n",
    "We use the `csr_matrix` class from the `scipy.sparse` module to represent the transaction matrix.\n",
    "This allows us to save memory by only storing the non-zero elements of the matrix. The `csr_matrix` class is a sparse matrix representation that stores the matrix in Compressed Sparse Row format and it is efficient for matrix-vector multiplication, which is how we will find the frequent itemsets of size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ALL_USERS_DATA = \"data/processed/user_data_cleaned.csv\"\n",
    "PATH_TRAIN_USERS_IDS = \"data/processed/train_users.csv\"\n",
    "PATH_TRAIN_USERS_DATA = \"data/processed/user_data_clean_train.csv\" # this will be created by the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_train_users(path_to_all_users_data: str, path_to_train_users_IDs: str, output_path: str) -> pd.DataFrame:\n",
    "    \n",
    "    # Read the data files\n",
    "    listened_songs_all_users = pd.read_csv(path_to_all_users_data)\n",
    "    train_users = pd.read_csv(path_to_train_users_IDs)[\"user_id\"]\n",
    "    # Filter the dataframe to include only users in the training set\n",
    "    train_users_data = listened_songs_all_users[listened_songs_all_users[\"user_id\"].isin(train_users)]\n",
    "    # Save the filtered dataframe    \n",
    "    train_users_data.to_csv(output_path, index=False)\n",
    "    # Print the number of unique users\n",
    "    print(f\"Number of unique users in the training set: {train_users_data['user_id'].nunique()}\")\n",
    "    print(f\"Number of unique songs in the training set: {train_users_data['song_id'].nunique()}\")\n",
    "    return train_users_data   \n",
    "\n",
    "def create_sparse_matrix(path_to_train_users_data: str, transaction_matrix: bool = True):\n",
    "    \"\"\"\n",
    "    Create a sparse matrix from a csv file with user data with columns song_id, user_id, play_count    \n",
    "    Input:\n",
    "    path_to_user_data: str, path to the user data\n",
    "    transaction_matrix: bool, if True the play_count is set to 1, else the play_count is used as is\n",
    "    Returns:\n",
    "    sparse_matrix: csr_matrix, \n",
    "    user_id_mapping: dict, mapping from the integer codes to the original user ids\n",
    "    song_id_mapping: dict, mapping from the integer codes to the original song ids\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path_to_train_users_data)\n",
    "    # Convert user_id and song_id to categories\n",
    "    # this the pandas way of mapping strings as integers    \n",
    "    df[\"user_id\"] = df[\"user_id\"].astype(\"category\")\n",
    "    df[\"song_id\"] = df[\"song_id\"].astype(\"category\")\n",
    "\n",
    "    # Extract the integer codes from the categories\n",
    "    user_codes = df[\"user_id\"].cat.codes\n",
    "    song_codes = df[\"song_id\"].cat.codes\n",
    "\n",
    "    # mapping from the integer codes to the original strings\n",
    "    user_id_mapping = dict(enumerate(df[\"user_id\"].cat.categories))\n",
    "    song_id_mapping = dict(enumerate(df[\"song_id\"].cat.categories))\n",
    "\n",
    "    \n",
    "    if transaction_matrix:\n",
    "        df[\"play_count\"] = 1  \n",
    "        # create of type bool to save memory      \n",
    "        sparse_matrix = csr_matrix(\n",
    "            (df[\"play_count\"], (user_codes, song_codes)),\n",
    "            shape=(df[\"user_id\"].cat.categories.size, df[\"song_id\"].cat.categories.size),\n",
    "            dtype=\"bool\"\n",
    "        )\n",
    "    else:\n",
    "        sparse_matrix = csr_matrix(\n",
    "            (df[\"play_count\"], (user_codes, song_codes)),\n",
    "            shape=(df[\"user_id\"].cat.categories.size, df[\"song_id\"].cat.categories.size),\n",
    "            dtype=\"int32\"\n",
    "        )\n",
    "        \n",
    "    print(f\"Created sparse matrix wih shape: {sparse_matrix.shape}\")\n",
    "    print(f\"Memory usage of sparse matrix: {sparse_matrix.data.nbytes / 1024:.2f} KB\")\n",
    "    return sparse_matrix , user_id_mapping, song_id_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in the training set: 156236\n",
      "Number of unique songs in the training set: 62066\n",
      "Created sparse matrix wih shape: (156236, 62066)\n",
      "Memory usage of sparse matrix: 6432.13 KB\n"
     ]
    }
   ],
   "source": [
    "train_users_data = find_train_users(PATH_ALL_USERS_DATA, PATH_TRAIN_USERS_IDS, PATH_TRAIN_USERS_DATA)\n",
    "transaction_matrix, user_id_mapping, song_id_mapping = create_sparse_matrix(PATH_TRAIN_USERS_DATA, transaction_matrix=True)\n",
    "TOTAL_NUMBER_OF_USERS = transaction_matrix.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Apriori Algorithm\n",
    "\n",
    "Our implementation of the Apriori Algorithm to find frequent itemsets of size 2 is as follows:\n",
    "\n",
    "1. Find the frequent singletons by summing the columns of the transaction matrix and remove the columns of the items which have a support less than the minimum support threshold. Save the frequent singletons.\n",
    "\n",
    "2. Let A be the transaction matrix only containing the frequent singletons. Now take the matrix product of A and its transpose. The result of this matrix multiplication is a matrix where the element at position (i, j) is the number of transactions where both item i and item j are present. To select the frequent itemsets of size 2, we can extract the row and column indices of the elements that are greater than or equal to the minimum support threshold. Save the frequent itemsets of size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_algorithm_pairs(crs_matrix: csr_matrix, min_support: float, write_to_file: bool = True):\n",
    "                            \n",
    "    \"\"\"\n",
    "    Apriori algorithm for pairs using crs_matrix and matrix multiplication\n",
    "    Input: \n",
    "    crs_matrix: sparse matrix, the transaction matrix\n",
    "    min_support: float, the minimum support\n",
    "    write_to_file: bool, if True the frequent itemsets are written to a file\n",
    "    \"\"\"\n",
    "    TOTAL_NUMBER_OF_USERS = crs_matrix.shape[0]\n",
    "    SUPPORT_THRESHOLD = int(math.ceil(TOTAL_NUMBER_OF_USERS * min_support))\n",
    "    WRITE_TO_FILE = write_to_file\n",
    "    out_folder = \"results\"\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "    OUT_PATH = f\"{out_folder}/frequent_itemsets_support_{min_support}.csv\"\n",
    "    # delete the file if it already exists\n",
    "    if WRITE_TO_FILE and os.path.exists(OUT_PATH):\n",
    "        os.remove(OUT_PATH)  \n",
    "    \n",
    "    # singletons           \n",
    "    single_item_occurance = np.array(crs_matrix.sum(axis=0)).flatten()\n",
    "    relevant_single_item_indices = np.where(single_item_occurance >= SUPPORT_THRESHOLD)[0]\n",
    "    print(f\"Number of single items above the threshold: {len(relevant_single_item_indices)}\")\n",
    "    filtered_csr_idx_to_org_item_idx = {new_index: old_index for new_index, old_index in enumerate(relevant_single_item_indices)}\n",
    "    filtered_crs_matrix = crs_matrix[:,relevant_single_item_indices]\n",
    "    # remove old matrix to free up memory\n",
    "    frequent_singletons = {i: single_item_occurance[i] for i in relevant_single_item_indices}\n",
    "\n",
    "    if WRITE_TO_FILE:\n",
    "        with open(OUT_PATH, \"a\") as f:                \n",
    "            for singleton, frequency in frequent_singletons.items():\n",
    "                support = frequency / TOTAL_NUMBER_OF_USERS                             \n",
    "                f.write(f\"({singleton}),{support}\\n\")\n",
    "\n",
    "    # pairs\n",
    "    pairwise_occurance_matrix = filtered_crs_matrix.astype(np.int32).T @ filtered_crs_matrix.astype(np.int32)\n",
    "    # only extract the lower triangle (excluding the diagonal) since the matrix is symmetric\n",
    "    pairwise_occurance_matrix = tril(pairwise_occurance_matrix, k=-1)\n",
    "    coo = pairwise_occurance_matrix.tocoo()\n",
    "    mask = coo.data >= SUPPORT_THRESHOLD\n",
    "    filtered_rows, filtered_cols, filtered_values = coo.row[mask], coo.col[mask], coo.data[mask]\n",
    "\n",
    "    original_rows = [filtered_csr_idx_to_org_item_idx[i] for i in filtered_rows]\n",
    "    original_cols = [filtered_csr_idx_to_org_item_idx[i] for i in filtered_cols]    \n",
    "\n",
    "    frequent_pairs  = {}\n",
    "    for row, col, value in zip(original_rows, original_cols, filtered_values):\n",
    "        item_set = tuple(sorted((row, col)))\n",
    "        assert item_set not in frequent_pairs, f\"Key {item_set} is already in the dictionary, should not happen\"            \n",
    "        frequent_pairs[item_set] = value\n",
    "\n",
    "    print(f\"Number of pairs above the threshold: {len(frequent_pairs)}\")\n",
    "    if WRITE_TO_FILE:\n",
    "        with open(OUT_PATH, \"a\") as f:                \n",
    "            for itemset in frequent_pairs:\n",
    "                # Ensure valid mapping of filtered indices to original item indices\n",
    "                #original_item_indices = tuple(sorted([filtered_csr_idx_to_org_item_idx[i] for i in itemset]))                   \n",
    "                support = frequent_pairs[itemset] / TOTAL_NUMBER_OF_USERS\n",
    "                f.write(f\"{itemset},{support}\\n\")        \n",
    "\n",
    "    return frequent_singletons, frequent_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of single items above the threshold: 9228\n",
      "Number of pairs above the threshold: 47748\n"
     ]
    }
   ],
   "source": [
    "MIN_SUPPORT = 0.001 # have been run with 0.001, 0,0005, 0.0001, 0.00005\n",
    "freq_singletons, freq_pairs = apriori_algorithm_pairs(transaction_matrix, min_support=MIN_SUPPORT, write_to_file=True)\n",
    "\n",
    "support_singletons = {item: freq_singletons[item] / TOTAL_NUMBER_OF_USERS for item in freq_singletons}\n",
    "support_pairs = {item: freq_pairs[item] / TOTAL_NUMBER_OF_USERS for item in freq_pairs}\n",
    "\n",
    "singleton_df = pd.DataFrame({\"item\": list(support_singletons.keys()), \"support\": list(support_singletons.values())})\n",
    "pairs_df = pd.DataFrame({\"itemset\": list(support_pairs.keys()), \"pair_support\": list(support_pairs.values())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence(support_A_and_B, support_A):\n",
    "    return support_A_and_B / support_A\n",
    "\n",
    "def calculate_lift(confidence_A_to_B, support_B):\n",
    "    return confidence_A_to_B / support_B\n",
    "\n",
    "pairs_df[\"item_A\"] = pairs_df[\"itemset\"].apply(lambda x: x[0])\n",
    "pairs_df[\"item_B\"] = pairs_df[\"itemset\"].apply(lambda x: x[1])\n",
    "pairs_df[\"support_A\"] = pairs_df[\"item_A\"].apply(lambda x: support_singletons[x])\n",
    "pairs_df[\"support_B\"] = pairs_df[\"item_B\"].apply(lambda x: support_singletons[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemset</th>\n",
       "      <th>pair_support</th>\n",
       "      <th>item_A</th>\n",
       "      <th>item_B</th>\n",
       "      <th>support_A</th>\n",
       "      <th>support_B</th>\n",
       "      <th>confidence_A_to_B</th>\n",
       "      <th>confidence_B_to_A</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(14, 55080)</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>14</td>\n",
       "      <td>55080</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>0.572973</td>\n",
       "      <td>0.157153</td>\n",
       "      <td>66.359530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(15, 42101)</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>15</td>\n",
       "      <td>42101</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.309951</td>\n",
       "      <td>97.829321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(15, 58716)</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>15</td>\n",
       "      <td>58716</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.361616</td>\n",
       "      <td>0.389130</td>\n",
       "      <td>122.820571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(18, 39893)</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>18</td>\n",
       "      <td>39893</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.146378</td>\n",
       "      <td>0.280401</td>\n",
       "      <td>32.717449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(18, 53681)</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>18</td>\n",
       "      <td>53681</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>0.194922</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>67.675041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       itemset  pair_support  item_A  item_B  support_A  support_B   \n",
       "0  (14, 55080)      0.001357      14   55080   0.002368   0.008634  \\\n",
       "1  (15, 42101)      0.001216      15   42101   0.003168   0.003924   \n",
       "2  (15, 58716)      0.001146      15   58716   0.003168   0.002944   \n",
       "3  (18, 39893)      0.001255      18   39893   0.008570   0.004474   \n",
       "4  (18, 53681)      0.001671      18   53681   0.008570   0.002880   \n",
       "\n",
       "   confidence_A_to_B  confidence_B_to_A        lift  \n",
       "0           0.572973           0.157153   66.359530  \n",
       "1           0.383838           0.309951   97.829321  \n",
       "2           0.361616           0.389130  122.820571  \n",
       "3           0.146378           0.280401   32.717449  \n",
       "4           0.194922           0.580000   67.675041  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df[\"confidence_A_to_B\"] = pairs_df.apply(\n",
    "    lambda x: calculate_confidence(x[\"pair_support\"], x[\"support_A\"]), axis=1\n",
    ")\n",
    "pairs_df[\"confidence_B_to_A\"] = pairs_df.apply(\n",
    "    lambda x: calculate_confidence(x[\"pair_support\"], x[\"support_B\"]), axis=1)\n",
    "\n",
    "pairs_df[\"lift\"] = pairs_df.apply(\n",
    "    lambda x: calculate_lift(x[\"confidence_A_to_B\"], x[\"support_B\"]), axis=1\n",
    ")\n",
    "pairs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved measurements to results/measurements_0.001.json\n"
     ]
    }
   ],
   "source": [
    "# save to json file:\n",
    "measurement_dict = {}\n",
    "\n",
    "for _, row in pairs_df.iterrows():\n",
    "    item1, item2 = song_id_mapping[row['item_A']], song_id_mapping[row['item_B']]\n",
    "    conf1_to_2 = row['confidence_A_to_B']\n",
    "    conf2_to_1 = row['confidence_B_to_A']\n",
    "    lift = row['lift'] # remember lift_A_to_B == lift_B_to_A    \n",
    "    # Add item1 -> item2 confidence\n",
    "    if item1 not in measurement_dict:\n",
    "        measurement_dict[item1] = {item2: {\"confidence\": conf1_to_2, \"lift\": lift}}   \n",
    "    else:\n",
    "        measurement_dict[item1][item2] = {\"confidence\": conf1_to_2, \"lift\": lift}\n",
    "    \n",
    "    # Add item2 -> item1 confidence\n",
    "    if item2 not in measurement_dict:\n",
    "        measurement_dict[item2] = {item1: {\"confidence\": conf2_to_1, \"lift\": lift}}   \n",
    "    else:\n",
    "        measurement_dict[item2][item1] = {\"confidence\": conf2_to_1, \"lift\": lift}\n",
    "\n",
    "# Save to JSON file\n",
    "with open(f\"results/measurements_{MIN_SUPPORT}.json\", \"w\") as json_file:\n",
    "    json.dump(measurement_dict, json_file, indent=4)\n",
    "\n",
    "print(f\"Saved measurements to results/measurements_{MIN_SUPPORT}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructure the data such that the confidence and lift values are sorted in descending order\n",
    "\n",
    "def restructure_data(data):\n",
    "    result = {}\n",
    "    \n",
    "    for song, related_songs in data.items():\n",
    "        # Collect confidence and lift values for each song\n",
    "        confidence_list = sorted(\n",
    "            [(related, values['confidence']) for related, values in related_songs.items()],\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        lift_list = sorted(\n",
    "            [(related, values['lift']) for related, values in related_songs.items()],\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        \n",
    "        result[song] = {\n",
    "            'confidence': confidence_list,\n",
    "            'lift': lift_list\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "restructured_data = restructure_data(measurement_dict)\n",
    "# save to json file:\n",
    "with open(f\"results/measurements_restructured_{MIN_SUPPORT}.json\", \"w\") as json_file:\n",
    "    json.dump(restructured_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Recommendations\n",
    "\n",
    "To recommend songs to users based on the songs they have listened to, we use the association rules generated by the Apriori Algorithm. We use either confidence or lift metrics to generate the recommendations. For a given test user we have a list of songs that the user has listened to. Based on the users listened songs, we generate 50 song recommendations as follows:\n",
    "\n",
    "1. For each song the user has listend to, we extract the songs that are associated with the listened song based on either confidence or lift values.\n",
    "\n",
    "2. Remove the songs that the user has already listened to. Sort the remaining songs based on the confidence or lift values and select the top 50 songs as recommendations.\n",
    "\n",
    "Finally the recommendations are evaluated using already defined evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TEST_DATA_FIXED = \"data/processed/listened_songs_fixed_split.csv\"\n",
    "PATH_TEST_MASKED_FIXED = \"data/processed/masked_songs_fixed_split.csv\"\n",
    "\n",
    "\n",
    "test_users_data_fixed = pd.read_csv(PATH_TEST_DATA_FIXED)\n",
    "test_users_data_masked_fixed = pd.read_csv(PATH_TEST_MASKED_FIXED)\n",
    "\n",
    "\n",
    "def group_data(data):\n",
    "    \"\"\"\n",
    "    Group the data by user_id\n",
    "    Input:\n",
    "    data: pd.DataFrame, the data to group\n",
    "    Returns:\n",
    "    grouped_data: dict, the grouped data\n",
    "    \"\"\"\n",
    "    grouped_data = data.groupby(\"user_id\")[\"song_id\"].apply(list).reset_index()\n",
    "    grouped_data.columns = ['user_id', 'songs']\n",
    "    return grouped_data\n",
    "\n",
    "test_users_data_fixed = group_data(test_users_data_fixed)\n",
    "test_users_data_masked_fixed = group_data(test_users_data_masked_fixed)\n",
    "test_users_data_fixed = test_users_data_fixed.merge(test_users_data_masked_fixed, on=\"user_id\", suffixes=('_listened', '_masked'))\n",
    "test_users_data_fixed[\"num_songs_to_recommend\"] = test_users_data_fixed[\"songs_masked\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs(restructured_data,known_songs,k,measurement_type='confidence',verbose=False):\n",
    "    \"\"\"\n",
    "    Recommend k songs based on the known songs\n",
    "    Input:\n",
    "    restructured_data: dict, the restructured data that is sorted in descending order\n",
    "    known_songs: list, the songs the user has listened to\n",
    "    k: int, the number of songs to recommend\n",
    "    measurement_type: str, the type of measurement to use, either 'confidence' or 'lift'\n",
    "    Returns:\n",
    "    recommended_songs: list, the recommended songs\n",
    "    \"\"\"\n",
    "    # Collect the confidence values for the known songs    \n",
    "    all_possible_recommendations = []\n",
    "    for song in known_songs:\n",
    "        try:\n",
    "            related_songs = restructured_data[song][measurement_type]\n",
    "            # remove the songs that are already known\n",
    "            related_songs = [(related, confidence) for related, confidence in related_songs if related not in known_songs]\n",
    "            all_possible_recommendations.extend(related_songs)\n",
    "        except KeyError:\n",
    "            if verbose:\n",
    "                print(f\"Song {song} is not in the dataset\")\n",
    "            continue\n",
    "    # Sort the confidence values in descending order\n",
    "    all_possible_recommendations = sorted(all_possible_recommendations, key=lambda x: x[1], reverse=True)\n",
    "    # Collect the top k songs\n",
    "    recommended_songs = [song_and_score for song_and_score in all_possible_recommendations[:k]]\n",
    "    return recommended_songs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_remove_cols(df_in):\n",
    "    #Make a copy of the dataframe\n",
    "    df = df_in.copy()\n",
    "    df[\"recommended_songs\"] = df[\"recommended_songs_tuple\"].apply(lambda x: [song for song, _ in x])\n",
    "    df[\"scores\"] = df[\"recommended_songs_tuple\"].apply(lambda x: [confidence for _, confidence in x])\n",
    "    df[\"probabilities\"] = df[\"scores\"].apply(lambda x: np.exp(x) / np.sum(np.exp(x)))\n",
    "    # keep columns ['user_id', 'recommended_songs', 'recommended_songs_score', 'softmax_score']\n",
    "    df = df[['user_id', 'recommended_songs', 'scores', 'probabilities']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurement type: confidence\n",
      "Measurement type: lift\n"
     ]
    }
   ],
   "source": [
    "# recommend songs for different measurement types and recommendation strategies\n",
    "# also evaluate the recommendations\n",
    "NUM_SONGS_TO_RECOMMEND = 50\n",
    "\n",
    "test_labels_masked_fixed = pd.read_csv(PATH_TEST_MASKED_FIXED)\n",
    "\n",
    "for measurement_type in ['confidence', 'lift']:    \n",
    "    print(f\"Measurement type: {measurement_type}\")    \n",
    "    test_users_data_fixed[\"recommended_songs_tuple\"] = test_users_data_fixed.apply(\n",
    "        lambda row: recommend_songs(restructured_data, \n",
    "                                    row[\"songs_listened\"],\n",
    "                                    NUM_SONGS_TO_RECOMMEND,                                       \n",
    "                                    measurement_type=measurement_type), axis=1)                                    \n",
    "    output_fixed = add_and_remove_cols(test_users_data_fixed)\n",
    "    output_fixed.to_csv(f\"results/recommendations_fixed_{measurement_type}_{MIN_SUPPORT}.csv\", index=False)\n",
    "    evaluation_fixed = evaluate_recommendations(output_fixed, \n",
    "                                                test_labels_masked_fixed) \n",
    "                                                \n",
    "    evaluation_fixed.to_csv(f\"results/evaluation_fixed_{measurement_type}_{MIN_SUPPORT}.csv\", index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>support</th>\n",
       "      <th>k_10</th>\n",
       "      <th>k_20</th>\n",
       "      <th>k_30</th>\n",
       "      <th>k_40</th>\n",
       "      <th>k_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>confidence</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.156953</td>\n",
       "      <td>0.224492</td>\n",
       "      <td>0.268784</td>\n",
       "      <td>0.301309</td>\n",
       "      <td>0.326465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lift</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.067148</td>\n",
       "      <td>0.104904</td>\n",
       "      <td>0.134564</td>\n",
       "      <td>0.158955</td>\n",
       "      <td>0.179813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confidence</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.141647</td>\n",
       "      <td>0.198633</td>\n",
       "      <td>0.235547</td>\n",
       "      <td>0.262005</td>\n",
       "      <td>0.282497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lift</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.080752</td>\n",
       "      <td>0.122320</td>\n",
       "      <td>0.153533</td>\n",
       "      <td>0.179243</td>\n",
       "      <td>0.200218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>confidence</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.124953</td>\n",
       "      <td>0.172523</td>\n",
       "      <td>0.202466</td>\n",
       "      <td>0.223498</td>\n",
       "      <td>0.240033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lift</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.081387</td>\n",
       "      <td>0.122244</td>\n",
       "      <td>0.151864</td>\n",
       "      <td>0.174304</td>\n",
       "      <td>0.191843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       method  support      k_10      k_20      k_30      k_40      k_50\n",
       "0  confidence   0.0001  0.156953  0.224492  0.268784  0.301309  0.326465\n",
       "1        lift   0.0001  0.067148  0.104904  0.134564  0.158955  0.179813\n",
       "2  confidence   0.0005  0.141647  0.198633  0.235547  0.262005  0.282497\n",
       "3        lift   0.0005  0.080752  0.122320  0.153533  0.179243  0.200218\n",
       "4  confidence   0.0010  0.124953  0.172523  0.202466  0.223498  0.240033\n",
       "5        lift   0.0010  0.081387  0.122244  0.151864  0.174304  0.191843"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supports = [0.001, 0.0005, 0.0001]\n",
    "methods = ['confidence', 'lift']\n",
    "\n",
    "data = []\n",
    "for support in supports:\n",
    "    for method in methods:\n",
    "        file = f\"results/evaluation_fixed_{method}_{support}.csv\"\n",
    "        mean_values = pd.read_csv(file).drop(columns=[\"user_id\"]).mean()\n",
    "        mean_values[\"method\"] = method\n",
    "        mean_values[\"support\"] = support\n",
    "        data.append(mean_values)\n",
    "        \n",
    "#final creation\n",
    "final_df = pd.DataFrame(data).reset_index(drop=True)\n",
    "# sort by support and method\n",
    "final_df = final_df.sort_values(by=[\"support\", \"method\"]).reset_index(drop=True)\n",
    "# make method and support the first columns\n",
    "final_df = final_df[[\"method\", \"support\"] + [col for col in final_df.columns if col not in [\"method\", \"support\"]]]\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComSocSci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
